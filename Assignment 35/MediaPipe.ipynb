{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "s3E6NFV-00Qt"
      },
      "outputs": [],
      "source": [
        "#@markdown To better demonstrate the Pose Landmarker API, we have created a set of visualization tools that will be used in this colab. These will draw the landmarks on a detect person, as well as the expected connections between those markers.\n",
        "\n",
        "from mediapipe import solutions\n",
        "from mediapipe.framework.formats import landmark_pb2\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def draw_landmarks_on_image(rgb_image, detection_result):\n",
        "  pose_landmarks_list = detection_result.pose_landmarks\n",
        "  annotated_image = np.copy(rgb_image)\n",
        "\n",
        "  # Loop through the detected poses to visualize.\n",
        "  for idx in range(len(pose_landmarks_list)):\n",
        "    pose_landmarks = pose_landmarks_list[idx]\n",
        "\n",
        "    # Draw the pose landmarks.\n",
        "    pose_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
        "    pose_landmarks_proto.landmark.extend([\n",
        "      landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in pose_landmarks\n",
        "    ])\n",
        "    solutions.drawing_utils.draw_landmarks(\n",
        "      annotated_image,\n",
        "      pose_landmarks_proto,\n",
        "      solutions.pose.POSE_CONNECTIONS,\n",
        "      solutions.drawing_styles.get_default_pose_landmarks_style())\n",
        "  return annotated_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iy4r2_ePylIa"
      },
      "source": [
        "## Running inference and visualizing the results\n",
        "\n",
        "The final step is to run pose landmark detection on your selected image. This involves creating your PoseLandmarker object, loading your image, running detection, and finally, the optional step of displaying the image with visualizations.\n",
        "\n",
        "Check out the [MediaPipe documentation](https://developers.google.com/mediapipe/solutions/vision/pose_landmarker/python) to learn more about configuration options that this solution supports.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "_JVO3rvPD4RN"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Unable to open file at e:\\python\\python_image_processing\\Assignment 35\\pose_landmarker.task",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[9], line 11\u001b[0m\n\u001b[0;32m      7\u001b[0m base_options \u001b[38;5;241m=\u001b[39m python\u001b[38;5;241m.\u001b[39mBaseOptions(model_asset_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpose_landmarker.task\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m options \u001b[38;5;241m=\u001b[39m vision\u001b[38;5;241m.\u001b[39mPoseLandmarkerOptions(\n\u001b[0;32m      9\u001b[0m     base_options\u001b[38;5;241m=\u001b[39mbase_options,\n\u001b[0;32m     10\u001b[0m     output_segmentation_masks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 11\u001b[0m detector \u001b[38;5;241m=\u001b[39m \u001b[43mvision\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPoseLandmarker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_from_options\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# STEP 3: Load the input image.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m image \u001b[38;5;241m=\u001b[39m mp\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mcreate_from_file(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput/ww.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\mediapipe\\tasks\\python\\vision\\pose_landmarker.py:319\u001b[0m, in \u001b[0;36mPoseLandmarker.create_from_options\u001b[1;34m(cls, options)\u001b[0m\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/mediapipe/tasks/python/vision/pose_landmarker.py?line=305'>306</a>\u001b[0m   output_streams\u001b[39m.\u001b[39mappend(\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/mediapipe/tasks/python/vision/pose_landmarker.py?line=306'>307</a>\u001b[0m       \u001b[39m'\u001b[39m\u001b[39m:\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin([_SEGMENTATION_MASK_TAG, _SEGMENTATION_MASK_STREAM_NAME])\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/mediapipe/tasks/python/vision/pose_landmarker.py?line=307'>308</a>\u001b[0m   )\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/mediapipe/tasks/python/vision/pose_landmarker.py?line=309'>310</a>\u001b[0m task_info \u001b[39m=\u001b[39m _TaskInfo(\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/mediapipe/tasks/python/vision/pose_landmarker.py?line=310'>311</a>\u001b[0m     task_graph\u001b[39m=\u001b[39m_TASK_GRAPH_NAME,\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/mediapipe/tasks/python/vision/pose_landmarker.py?line=311'>312</a>\u001b[0m     input_streams\u001b[39m=\u001b[39m[\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/mediapipe/tasks/python/vision/pose_landmarker.py?line=316'>317</a>\u001b[0m     task_options\u001b[39m=\u001b[39moptions,\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/mediapipe/tasks/python/vision/pose_landmarker.py?line=317'>318</a>\u001b[0m )\n\u001b[1;32m--> <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/mediapipe/tasks/python/vision/pose_landmarker.py?line=318'>319</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m(\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/mediapipe/tasks/python/vision/pose_landmarker.py?line=319'>320</a>\u001b[0m     task_info\u001b[39m.\u001b[39;49mgenerate_graph_config(\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/mediapipe/tasks/python/vision/pose_landmarker.py?line=320'>321</a>\u001b[0m         enable_flow_limiting\u001b[39m=\u001b[39;49moptions\u001b[39m.\u001b[39;49mrunning_mode\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/mediapipe/tasks/python/vision/pose_landmarker.py?line=321'>322</a>\u001b[0m         \u001b[39m==\u001b[39;49m _RunningMode\u001b[39m.\u001b[39;49mLIVE_STREAM\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/mediapipe/tasks/python/vision/pose_landmarker.py?line=322'>323</a>\u001b[0m     ),\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/mediapipe/tasks/python/vision/pose_landmarker.py?line=323'>324</a>\u001b[0m     options\u001b[39m.\u001b[39;49mrunning_mode,\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/mediapipe/tasks/python/vision/pose_landmarker.py?line=324'>325</a>\u001b[0m     packets_callback \u001b[39mif\u001b[39;49;00m options\u001b[39m.\u001b[39;49mresult_callback \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/mediapipe/tasks/python/vision/pose_landmarker.py?line=325'>326</a>\u001b[0m )\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\mediapipe\\tasks\\python\\vision\\core\\base_vision_task_api.py:70\u001b[0m, in \u001b[0;36mBaseVisionTaskApi.__init__\u001b[1;34m(self, graph_config, running_mode, packet_callback)\u001b[0m\n\u001b[0;32m     <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/mediapipe/tasks/python/vision/core/base_vision_task_api.py?line=64'>65</a>\u001b[0m \u001b[39melif\u001b[39;00m packet_callback:\n\u001b[0;32m     <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/mediapipe/tasks/python/vision/core/base_vision_task_api.py?line=65'>66</a>\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m     <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/mediapipe/tasks/python/vision/core/base_vision_task_api.py?line=66'>67</a>\u001b[0m       \u001b[39m'\u001b[39m\u001b[39mThe vision task is in image or video mode, a user-defined result \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/mediapipe/tasks/python/vision/core/base_vision_task_api.py?line=67'>68</a>\u001b[0m       \u001b[39m'\u001b[39m\u001b[39mcallback should not be provided.\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/mediapipe/tasks/python/vision/core/base_vision_task_api.py?line=68'>69</a>\u001b[0m   )\n\u001b[1;32m---> <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/mediapipe/tasks/python/vision/core/base_vision_task_api.py?line=69'>70</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_runner \u001b[39m=\u001b[39m _TaskRunner\u001b[39m.\u001b[39;49mcreate(graph_config, packet_callback)\n\u001b[0;32m     <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/mediapipe/tasks/python/vision/core/base_vision_task_api.py?line=70'>71</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_running_mode \u001b[39m=\u001b[39m running_mode\n",
            "\u001b[1;31mRuntimeError\u001b[0m: Unable to open file at e:\\python\\python_image_processing\\Assignment 35\\pose_landmarker.task"
          ]
        }
      ],
      "source": [
        "# STEP 1: Import the necessary modules.\n",
        "import mediapipe as mp\n",
        "from mediapipe.tasks import python\n",
        "from mediapipe.tasks.python import vision\n",
        "\n",
        "# STEP 2: Create an PoseLandmarker object.\n",
        "base_options = python.BaseOptions(model_asset_path='pose_landmarker.task')\n",
        "options = vision.PoseLandmarkerOptions(\n",
        "    base_options=base_options,\n",
        "    output_segmentation_masks=True)\n",
        "detector = vision.PoseLandmarker.create_from_options(options)\n",
        "\n",
        "# STEP 3: Load the input image.\n",
        "image = mp.Image.create_from_file(\"input/ww.jpg\")\n",
        "\n",
        "# STEP 4: Detect pose landmarks from the input image.\n",
        "detection_result = detector.detect(image)\n",
        "\n",
        "# STEP 5: Process the detection result. In this case, visualize it.\n",
        "annotated_image = draw_landmarks_on_image(image.numpy_view(), detection_result)\n",
        "cv2_imshow(cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BwzFvaxwtPX"
      },
      "source": [
        "Visualize the pose segmentation mask."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jAIFzw9M3JJ"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'detection_result' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m segmentation_mask \u001b[38;5;241m=\u001b[39m \u001b[43mdetection_result\u001b[49m\u001b[38;5;241m.\u001b[39msegmentation_masks[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy_view()\n\u001b[0;32m      2\u001b[0m visualized_mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrepeat(segmentation_mask[:, :, np\u001b[38;5;241m.\u001b[39mnewaxis], \u001b[38;5;241m3\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m255\u001b[39m\n\u001b[0;32m      3\u001b[0m cv2_imshow(visualized_mask)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'detection_result' is not defined"
          ]
        }
      ],
      "source": [
        "segmentation_mask = detection_result.segmentation_masks[0].numpy_view()\n",
        "visualized_mask = np.repeat(segmentation_mask[:, :, np.newaxis], 3, axis=2) * 255\n",
        "cv2_imshow(visualized_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QipRi2ozw7cg"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
